# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfi21MuSz8FttrPvVsA2a5Nzi1FxyJuU
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install xgboost

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, log_loss
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/adult 3.csv")

df

#Shows how many rows and columns are present in dataset
df.shape

# Handle missing values
df.replace(' ?', np.nan, inplace=True)
df.dropna(inplace=True)

# Encode categorical columns
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Features and target
X = df.drop('income', axis=1)
y = df['income']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "SVM": SVC(probability=True)
}

# Train and evaluate models
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)
    acc = accuracy_score(y_test, y_pred)
    loss = log_loss(y_test, y_proba)
    results[name] = {"accuracy": acc, "loss": loss}

# Show results
print("Model Performance:\n")
for model in results:
    print(f"{model}: Accuracy = {results[model]['accuracy']:.4f}, Log Loss = {results[model]['loss']:.4f}")

# Plot results
model_names = list(results.keys())
accuracies = [results[m]['accuracy'] for m in model_names]
losses = [results[m]['loss'] for m in model_names]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.barplot(x=accuracies, y=model_names, palette='viridis')
plt.title('Model Accuracy Comparison')
plt.xlabel('Accuracy')
plt.xlim(0.7, 0.9)

plt.subplot(1, 2, 2)
sns.barplot(x=losses, y=model_names, palette='magma')
plt.title('Model Log Loss Comparison')
plt.xlabel('Log Loss')

plt.tight_layout()
plt.show()

#USING XGBOOST MODEL TO GAIN MORE ACCURACY
# STEP 1: Install XGBoost

!pip install xgboost

# STEP 2: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss
import xgboost as xgb

# STEP 3: Load Dataset
df = pd.read_csv('/content/drive/MyDrive/adult 3.csv')
df.replace(' ?', np.nan, inplace=True)
df.dropna(inplace=True)

# STEP 4: Encode Categorical Columns
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# STEP 5: Feature/Target Split & Scaling
X = df.drop("income", axis=1)
y = df["income"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# STEP 6: Convert to XGBoost DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# STEP 7: Define Parameters
params = {
    'objective': 'binary:logistic',
    'eval_metric': ['logloss', 'error'],
    'learning_rate': 0.1,
    'max_depth': 5,
    'seed': 42
}
evals_result = {}

# STEP 8: Train Model
bst = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=100,
    evals=[(dtrain, 'train'), (dtest, 'eval')],
    evals_result=evals_result,
    verbose_eval=False
)

# STEP 9: Plot Accuracy and Loss
# Accuracy = 1 - error

epochs = range(1, 101)
train_error = [1 - x for x in evals_result['train']['error']]
eval_error = [1 - x for x in evals_result['eval']['error']]
train_loss = evals_result['train']['logloss']
eval_loss = evals_result['eval']['logloss']

# Plot Accuracy
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_error, label='Train Accuracy')
plt.plot(epochs, eval_error, label='Validation Accuracy')
plt.title('XGBoost Accuracy over Rounds')
plt.xlabel('Boosting Rounds')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Loss
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_loss, label='Train Log Loss')
plt.plot(epochs, eval_loss, label='Validation Log Loss')
plt.title('XGBoost Log Loss over Rounds')
plt.xlabel('Boosting Rounds')
plt.ylabel('Log Loss')
plt.legend()
plt.grid(True)
plt.show()

# STEP 10: Final Evaluation
y_pred_prob = bst.predict(dtest)
y_pred = [1 if p > 0.5 else 0 for p in y_pred_prob]

final_acc = accuracy_score(y_test, y_pred)
final_loss = log_loss(y_test, y_pred_prob)

print(f"\nFinal Accuracy: {final_acc:.4f}")
print(f"Final Log Loss: {final_loss:.4f}")